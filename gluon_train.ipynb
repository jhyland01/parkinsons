{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/srwjqw_j5gsbl1y7xdb9jc900000gn/T/ipykernel_36251/1112237029.py:3: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  filtered_data = pd.read_csv('./processed.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_data = pd.read_csv('./processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and targets (y)\n",
    "X = filtered_data.drop(columns=['StartHesitation', 'Turn', 'Walking'])\n",
    "y = filtered_data[['StartHesitation', \n",
    "                   'Turn', \n",
    "                   'Walking',\n",
    "                  ]]\n",
    "\n",
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Combine the features and targets back into DataFrames for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "del filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230501_180317/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for StartHesitation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (6103075 samples, 663.09 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230501_180317/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.4\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.1.0: Sun Oct  9 20:14:30 PDT 2022; root:xnu-8792.41.9~2/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    6103075\n",
      "Train Data Columns: 7\n",
      "Label Column: StartHesitation\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    890.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 565.44 MB (63.5% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 63.5% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 5 | ['AccV', 'AccML', 'AccAP', 'Turn', 'Walking']\n",
      "\t\t('int', [])                        : 1 | ['Unnamed: 0']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Time']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 5 | ['AccV', 'AccML', 'AccAP', 'Turn', 'Walking']\n",
      "\t\t('int', [])                  : 1 | ['Unnamed: 0']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['Time', 'Time.year', 'Time.month', 'Time.day', 'Time.dayofweek']\n",
      "\t70.6s = Fit runtime\n",
      "\t7 features in original data used to generate 11 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 537.07 MB (33.8% of available memory)\n",
      "\tWarning: Data size post feature transformation consumes 33.8% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "Data preprocessing and feature engineering runtime = 72.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 6042044, Val Rows: 61031\n",
      "Excluded Model Types: ['LightGBMLarge']\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\tWarning: Model is expected to require 93.82% of available memory... (20.0% is the max safe size.)\n",
      "\tNot enough memory to train KNeighborsUnif... Skipping this model.\n",
      "Fitting model: KNeighborsDist ...\n",
      "\tWarning: Model is expected to require 118.07% of available memory... (20.0% is the max safe size.)\n",
      "\tNot enough memory to train KNeighborsDist... Skipping this model.\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 4.205 GB, but only 0.788 GB is available...\n",
      "\tNot enough memory to train LightGBMXT... Skipping this model.\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 4.205 GB, but only 0.797 GB is available...\n",
      "\tNot enough memory to train LightGBM... Skipping this model.\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tWarning: Model is expected to require 509.79% of available memory (Estimated before training)...\n",
      "\tNot enough memory to train RandomForestMSE... Skipping this model.\n",
      "Fitting model: CatBoost ...\n",
      "\tWarning: Not enough memory to safely train CatBoost model, roughly requires: 4.205 GB, but only 0.785 GB is available...\n",
      "\tNot enough memory to train CatBoost... Skipping this model.\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tWarning: Model is expected to require 519.15% of available memory (Estimated before training)...\n",
      "\tNot enough memory to train ExtraTreesMSE... Skipping this model.\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 5.8 GB, but only 0.79 GB is available...\n",
      "\tNot enough memory to train NeuralNetFastAI... Skipping this model.\n",
      "Fitting model: XGBoost ...\n",
      "\tWarning: Not enough memory to safely train XGBoost model, roughly requires: 4.205 GB, but only 0.791 GB is available...\n",
      "\tNot enough memory to train XGBoost... Skipping this model.\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 2.32 GB, but only 0.746 GB is available...\n",
      "\tNot enough memory to train NeuralNetTorch... Skipping this model.\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 4.205 GB, but only 0.75 GB is available...\n",
      "\tNot enough memory to train LightGBMLarge... Skipping this model.\n",
      "No base models to train on, skipping auxiliary stack level 2...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "AutoGluon did not successfully train any models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining model for \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     predictor \u001b[39m=\u001b[39m TabularPredictor(label\u001b[39m=\u001b[39mlabel, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                  problem_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                  eval_metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_absolute_error\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# regression with R^2 as the evaluation metric\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     predictor\u001b[39m.\u001b[39;49mfit(train_data, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                 \u001b[39m#   num_gpus=2, \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                   excluded_model_types\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mLightGBMLarge\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#                   time_limit=600, \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#                   num_bag_sets = 2, \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                 \u001b[39m#   hyperparameters = 'light',\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                   )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johnny/Library/CloudStorage/OneDrive-Personal/py/Kaggle/parkinsons/gluon_train.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     predictors[label] \u001b[39m=\u001b[39m predictor\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kaggle/lib/python3.10/site-packages/autogluon/core/utils/decorators.py:30\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mgargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kaggle/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:866\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m     aux_kwargs[\u001b[39m'\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m--> 866\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mtrain_data, X_val\u001b[39m=\u001b[39;49mtuning_data, X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[1;32m    867\u001b[0m                   holdout_frac\u001b[39m=\u001b[39;49mholdout_frac, num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds, num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[1;32m    868\u001b[0m                   num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[1;32m    869\u001b[0m                   hyperparameters\u001b[39m=\u001b[39;49mhyperparameters, core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs, aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[1;32m    870\u001b[0m                   time_limit\u001b[39m=\u001b[39;49mtime_limit, infer_limit\u001b[39m=\u001b[39;49minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    871\u001b[0m                   verbosity\u001b[39m=\u001b[39;49mverbosity, use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout)\n\u001b[1;32m    872\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[1;32m    875\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m'\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    876\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m'\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[1;32m    881\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kaggle/lib/python3.10/site-packages/autogluon/tabular/learner/abstract_learner.py:125\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 125\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, X_val\u001b[39m=\u001b[39;49mX_val, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kaggle/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:118\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m--> 118\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    119\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    120\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    121\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    122\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    123\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    124\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[1;32m    125\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit_trainer,\n\u001b[1;32m    126\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    127\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    128\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    129\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrainer_fit_kwargs\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[1;32m    132\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kaggle/lib/python3.10/site-packages/autogluon/tabular/trainer/auto_trainer.py:98\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[0;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_bag_holdout:\n\u001b[1;32m     86\u001b[0m         \u001b[39m# TODO: User could be intending to blend instead. Add support for blend stacking.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         \u001b[39m#  This error message is necessary because when calculating out-of-fold predictions for user, we want to return them in the form given in train_data,\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         \u001b[39m#  but if we merge train and val here, it becomes very confusing from a users perspective, especially because we reset index, making it impossible to match\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[39m#  the original train_data to the out-of-fold predictions from `predictor.get_oof_pred_proba()`.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mX_val, y_val is not None, but bagged mode was specified. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     91\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mIf calling from `TabularPredictor.fit()`, `tuning_data` should be None.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     92\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mDefault bagged mode does not use tuning data / validation data. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mspecify the following:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     96\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mpredictor.fit(..., tuning_data=tuning_data, use_bag_holdout=True)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m     99\u001b[0m                                y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    100\u001b[0m                                X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[1;32m    101\u001b[0m                                y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[1;32m    102\u001b[0m                                X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[1;32m    103\u001b[0m                                hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[1;32m    104\u001b[0m                                num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[1;32m    105\u001b[0m                                time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[1;32m    106\u001b[0m                                core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[1;32m    107\u001b[0m                                aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[1;32m    108\u001b[0m                                infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[1;32m    109\u001b[0m                                infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[1;32m    110\u001b[0m                                groups\u001b[39m=\u001b[39;49mgroups)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kaggle/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:2054\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[0;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[1;32m   2051\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_multi_levels(X, y, hyperparameters\u001b[39m=\u001b[39mhyperparameters, X_val\u001b[39m=\u001b[39mX_val, y_val\u001b[39m=\u001b[39my_val,\n\u001b[1;32m   2052\u001b[0m                                           X_unlabeled\u001b[39m=\u001b[39mX_unlabeled, level_start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, level_end\u001b[39m=\u001b[39mnum_stack_levels\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, time_limit\u001b[39m=\u001b[39mtime_limit, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   2053\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2054\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2055\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_fit\n",
      "\u001b[0;31mValueError\u001b[0m: AutoGluon did not successfully train any models"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import gc\n",
    "\n",
    "# Assuming 'train_data' and 'test_data' are DataFrames created from the previous response\n",
    "# Convert the data to AutoGluon's TabularDataset format\n",
    "train_data = TabularDataset(train_data)\n",
    "test_data = TabularDataset(test_data)\n",
    "\n",
    "# Define the target columns\n",
    "labels = ['StartHesitation', 'Turn', 'Walking']\n",
    "\n",
    "# Initialize an empty dictionary to store the predictors\n",
    "predictors = {}\n",
    "\n",
    "# Train a separate regression model for each target event type\n",
    "for label in labels:\n",
    "    print(f\"Training model for {label}...\")\n",
    "    predictor = TabularPredictor(label=label, \n",
    "                                 problem_type='regression', \n",
    "                                 eval_metric='mean_absolute_error') # regression with R^2 as the evaluation metric\n",
    "    predictor.fit(train_data, \n",
    "                #   num_gpus=2, \n",
    "                  excluded_model_types=['LightGBMLarge'], \n",
    "#                   time_limit=600, \n",
    "#                   num_bag_sets = 2, \n",
    "                #   hyperparameters = 'light',\n",
    "                  )\n",
    "\n",
    "    predictors[label] = predictor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for each target event type\n",
    "predictions = {}\n",
    "for label in labels:\n",
    "    print(f\"Predicting probabilities for {label}...\")\n",
    "    predictor = predictors[label]\n",
    "    predictions[label] = predictor.predict(test_data.drop(columns=labels))\n",
    "    \n",
    "\n",
    "# Combine the predictions into a single DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "print(predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
